## Elastic Net GLM

### Background

Elastic net regularization allows the modeler to combine both LASSO and ridge penalties into a single model. 

As one may recall, ordinary least squares regression requires minimizing the squared difference of the response variable and the predicted values. In symbols,

$$ \underset{\beta}{\arg\min} \sum_{n}(y-X\beta)^{2} $$

This is equivalent to maximum likelihood estimation, where one assumes that the response variable $y$ is normally distributed with mean $X\beta$ and variance $\sigma^{2}I_{k x k}$. The maximum is taken with respect to $\beta$, and the variance parameter is assumed to be fixed but unknown.

The LASSO and ridge regression methods each add an additional penalty term on the coefficients $\beta$. The LASSO adds the sum of the absolute values of the parameters $\beta$ subject to a tunable weight, $\lambda$. This term incentivizes the fitting algorithm to fit toward parameter values close to 0. 

$$ \underset{\beta}{\arg\min} \sum_{n}(y-X\beta)^{2}+ \lambda\sum_{k}|\beta_{k}|$$

The ridge penalty adds the sum of the squares of the parameters $\beta$, subject to a tunable weight, $\alpha$. This term also incentivizes the fitting algorithm to fit toward parameter values close to 0. 

$$ \underset{\beta}{\arg\min} \sum_{n}(y-X\beta)^{2}+ \alpha\sum_{k}\beta_{k}^{2}$$

What may be new to some readers is that in both cases, for special $\lambda$ or $\alpha$, the minimizers of these expressions correspond to the Bayesian maximum a posteriori (MAP) estimators for specific prior distributions for $\beta$. In the ridge case, the prior is the normal distribution with mean 0 and covariance $\tau^{2} I_{k \times k}$ for some assumed $\tau^{2}$. 

For the LASSO, the prior is the double-exponential or Laplace distribution with mean 0 and parameter $\tau$.

In either case, it can be shown that if $\sigma^{2}$ and $\tau$ are known, the penalizing weights have unique solutions and are equivalent to the $k$ term in B&uuml;hlmann credibility. In practice, the penalizing weights are unknown and must be tuned. The resulting optimal $\beta$ is also credible from a Bayesian perspective. Moreover, it can be shown that these facts carry over to the GLM case.

### Data Preparation

Elastic net GLMs as implemented in the glmnet package require that the inputs be converted to model matrices.

```{r glmnet-int-data-prep, echo = FALSE}
## Prepare data in matrices appropriately formatted for glmnet

## Determine the formula for glmnet based on interaction and LightGBM settings
if (nUseTopLightGBMInteractions == "ALL") {
  ## Use all interactions from LightGBM
  glmnetFormula <- as.formula(
    paste(c("~ -1", pred_cols, imp.int2[, Feature]), collapse = " + ")
  )
} else if (nUseTopLightGBMInteractions > 0) {
  ## Use the top N interactions from LightGBM
  glmnetFormula <- as.formula(
    paste(c("~ -1", pred_cols, imp.int2[1:nUseTopLightGBMInteractions, Feature]), collapse = " + ")
  )
} else if (nInteractionDepth > 1) {
  ## Use interactions up to the specified depth
  glmnetFormula <- as.formula(
    paste("~ -1 + (.)^", nInteractionDepth, sep = "")
  )
} else {
  ## Use only the main effects
  glmnetFormula <- as.formula(
    paste(c("~ -1", pred_cols), collapse = " + ")
  )
}

## Create model matrices for the training data
train.x.net <- model.Matrix(
  glmnetFormula, 
  train[, ..pred_cols], 
  sparse = bUseSparse
)
train.y.net <- as.matrix(train[, ..resp_var])
train.offset.net <- as.matrix(train[, ..resp_offset])

## Create model matrices for the test data
test.x.net <- model.matrix(
  glmnetFormula, 
  test[, ..pred_cols], 
  sparse = bUseSparse
)
test.y.net <- as.matrix(test[, ..resp_var])
test.offset.net <- as.matrix(test[, ..resp_offset])
```

### Model Fitting

Once the data are set up, we can calibrate a LASSO penalty, lambda, using n-fold cross-validation.

```{r glmnet-int-cv}
## In this section we fit a penalized linear regression model using glmnet. 
## We first fine-tune hyperparameter alpha using cross-validation
## Then we refit the model using the optimal alpha

#-----------------------------------------#
##### Perform Cross-Validation #####
#-----------------------------------------#
## Load or fit the cross-validated glmnet model

## Check if the cached model exists and is valid
if (bUseCache & file.exists(paste0(cacheFileRoot, "_glmnet_int_cv_model.rds")) & !bInvalidateCaches) {
  ## Load the cached model if it exists
  cvfit <- readRDS(paste0(cacheFileRoot, "_glmnet_int_cv_model.rds"))
} else {
  ## Initialize a cluster for parallel processing
  cl <- makeCluster(nGLMNetCores)
  registerDoParallel(cl)
  
  ## Set glmnet control options, enable iteration trace if debugging
  glmnet.control(itrace = ifelse(bDebug, 1, 0))
  
  ## Set random seed for reproducibility
  set.seed(nELSeed)
  
  ## Perform cross-validated glmnet model fitting
  cvfit <- cv.glmnet(
    train.x.net, 
    train.y.net, 
    offset = log(train.offset.net), 
    family = "poisson", 
    alpha = fGLMNetAlpha,
    parallel = TRUE
  )
  
  ## Stop the parallel cluster
  stopCluster(cl)
  
  ## Save the model to cache if caching is enabled
  if (bUseCache) {
    saveRDS(cvfit, paste0(cacheFileRoot, "_glmnet_int_cv_model.rds"))
  }
}

## Generate predictions for the test data (handling offsets correctly)
test[, predictions_glmnet := predict(
  cvfit,
  s = "lambda.min",
  newx = test.x.net, 
  type = "response", 
  newoffset = log(test.offset.net)
)]

## Generate predictions for the training data (handling offsets correctly)
train[, predictions_glmnet := predict(
  cvfit,
  s = "lambda.min",
  newx = train.x.net, 
  type = "response", 
  newoffset = log(train.offset.net)
)]
```

#### Model Plots and Tables {.tabset}

An important by-product of the n-fold cross validation is the plot of $\lambda$ values. The optimal choice of $\lambda$ is the lowest, and the model associated with that $\lambda$ is the final model.

One can also plot the trajectory of coefficients as progressively higher $\lambda$ imposes ever harsher penalties on the coefficients.

##### Lambda Plot

```{r glmnet-cvfit-lambdas}

## plot lambda vs deviance

data.table(lambda=cvfit$lambda,
           cvm=cvfit$cvm,
           cvlo=cvfit$cvlo,
           cvup=cvfit$cvup,
           nzero=cvfit$nzero) %>%
  ggplot(aes(x=log(lambda))) +
  geom_point(aes(y=cvm),color="red") +
  geom_errorbar(aes(ymin=cvlo,ymax=cvup),color="grey") +
  scale_y_continuous(name=cvfit$name,
                     limits = range(cvfit$cvup,cvfit$cvlo)) +
  scale_x_continuous(name=expression(Log(lambda))) +
  geom_vline(xintercept=log(cvfit$lambda.min),linetype=3) +
  geom_vline(xintercept = log(cvfit$lambda.1se),linetype=3) +
  theme_minimal()
```

##### Coefficient Penalization

```{r glmnet-cvfit-coeff-pen}
plot(cvfit$glmnet.fit,xvar="lambda")
```

##### Final Model

```{r glmnet-fit}
#---------------------------------------------------------------#
##### Examine model for minimum lambda and make predictions #####
#---------------------------------------------------------------#

## get coefficients for fitted version, reformatted and filtered
reformatCoefs(cvfit, pred_cols)  %>%
  filter(Coef != 0) %>%
  select(Feature1Name,
         Feature1Level,
         Feature2Name,
         Feature2Level,
         Coef) %>%
  mutate(Coef=exp(Coef)) %>%
  flextable() %>%
  highlight(j="Coef",color="yellow",part="body", i = ~ abs(log(Coef)) > log(1.05)) %>%
  set_formatter(
    Coef=function(x) paste0(sprintf("%.01f", 100*x),"%")
  ) %>%
  set_table_properties(opts_html=list(
        scroll=list(
          add_css="max-height: 500px;"
          )
        )
        ) %>%
  autofit() ->
  output_table

if(output_format == "html") {
  output_table
} else {
  wb <- wb_workbook()
  
  wb$add_worksheet("GLMNet Coefficients")
  
  wb <- wb_add_flextable(
    wb,
    "GLMNet Coefficients",
    output_table
  )
  
  wb$save(
    paste0(
      exportsRoot,
      "_glmnet_coefficients_table.xlsx"
    )
  )
  
  cat("See included Excel table for additional information.\n")
}

```

##### Lift

```{r glmnet-metrics-lift}

#-----------------------------------------#
##### Calculate Validation Metrics #####
#-----------------------------------------#

## generate plot
test[,decile.table(get(resp_var), predictions_glmnet/get(resp_offset), get(resp_offset))] %>%
  pivot_longer(-c(decile,exposures)) %>%
  as.data.table() %>%
  ggplot(aes(x=decile, y=value, col=name)) +  
  geom_line() +
  scale_x_continuous(breaks=c(1:10)) +
  labs(x="Decile",y="Deaths") +
  theme_minimal() +
  ggtitle("Decile Lift Plot") 

```

##### Lorenz Curve

```{r glmnet-metrics-lorenz}
## lorenz plot
test[,lorenz(get(resp_var), predictions_glmnet / get(resp_offset), get(resp_offset))]
```

### Plots of Terms {.tabset}

```{r glmnet-int-list}
## Generate a grid of predictor levels for the training data
train[, ..pred_cols] %>%
  lapply(levels) %>%
  expand.grid() %>%
  setDT() -> train.grid

## Create model matrix for the training grid
train.grid %>%
  model.Matrix(
    object = glmnetFormula,
    data = .,
    sparse = bUseSparse
  ) %>%
  ## Predict new coefficients using the fitted cv.glmnet model
  predict(
    cvfit,
    newx = .,
    s = "lambda.min",
    newoffset = rep(0, nrow(.))
  ) %>%
  as.vector() -> newCoef

## Add the predicted factors to the training grid
train.grid %>%
  add_column(Factor = exp(newCoef)) %>%
  setDT() -> train.grid

## Reformat coefficients and generate the list of interactions
reformatCoefs(cvfit, pred_cols) %>%
  filter(Coef != 0 & !is.na(Feature2Name)) %>%
  select(Feature1Name, Feature2Name) %>%
  distinct() %>%
  as.list() %>%
  purrr::list_transpose() -> glmnet.int.list
```

Below are plots of the 2-way interaction terms, with external factors fixed at their middle values.

```{r glmnet-term-plots, results='asis', message=FALSE, warning=FALSE}
## Generate and print plots for each interaction pair in glmnet.int.list
glmnet.int.list %>%
  map(.f = \(x) {
    ## Generate a plot of cross-validated net coefficients
    plotCVNetCoefs(
      train.grid,
      sort(x),
      "Factor",
      pred_cols
    )
  }) %>%
  ## Set names for each plot based on the interaction pairs
  purrr::set_names(
    map(glmnet.int.list, \(x) paste0(x, collapse = " x "))
  ) %>%
  ## Print each plot with a corresponding title
  iwalk(~ {
    cat('#### ', .y, '\n\n')  # Print the plot title
    print(.x)  # Print the plot
    cat('\n\n')  # Add spacing after each plot
  })
```

### Tables of Terms {.tabset}

Below are tables of 2-way interactions, with external factors fixed at their middle values.

```{r}
#| message: false
#| results: asis

## Generate and format summary tables for each interaction pair in glmnet.int.list
glmnet.int.list %>%
  map(.f = \(x) {
    # Generate a table of cross-validated net coefficients
    tableCVNetCoefs(
      train.grid,
      sort(x),
      "Factor",
      pred.cols = pred_cols
    ) %>%
      ## Create a flextable from the generated table
      flextable() %>%
      ## Format the table values, converting numeric values to percentages
      set_formatter(values = function(x) {
        if (is.numeric(x))
          sprintf("%.1f%%", x * 100)
        else
          x
      }) %>%
      ## Set a caption for the table
      set_caption(
        paste0("Factors for ", x[1], " and ", x[2], ", all other factors fixed at middle levels")
      ) %>%
      ## Set table properties to enable scrolling
      set_table_properties(opts_html = list(
        scroll = list(
          add_css = "max-height: 500px;"
        )
      )) %>%
      autofit() #%>%
    ## Print the flextable
    #knitr::knit_print()
  }) %>%
  ## Set names for each table based on the interaction pairs
  purrr::set_names(
    map(glmnet.int.list, \(x) paste0(x, collapse = " x "))
  ) -> 
  output_tables

if(output_format == "html") {
  output_tables %>%
    map(.f=knitr::knit_print) %>%
    ## Generate a tabset from the list of tables
    generate_tabset(
      tabtitle = "",
      tablevel = 3
    ) %>%
    ## Print the generated tabset
    cat()
} else {
  export_tables_to_excel(
    output_tables,
    paste0(
      exportsRoot,
      "_glmnet_int_twoway_interaction_tables.xlsx"
    )
  )
  
  cat("See included Excel table for additional information.\n")
}

```

### Goodness-of-Fit

Goodness-of-fit tables are provided. Each table provides actual-to-model ratios for single variables and for 2-way combinations of variables. A model is qualitatively deemed to perform well if goodness-of-fit ratios are close to 100% in almost all situations. The quantitative assessment using significance testing is omitted here.

#### Unvariate Goodness-of-Fit {.tabset}

```{r glmnet-gof-univariate, message=FALSE, warning=FALSE,results='asis'}
## Generate and format summary tables for each factor column
map(factor_cols, .f = \(x) {
  ## Convert column name to symbol for tidy evaluation
  x <- sym(x)
  resp_var_sym <- resp_var
  
  ## Summarize data by the current factor column
  train %>%
    group_by(!!x) %>%
    summarize(
      Outcome = sum(amount_actual),  # Sum the actual amounts
      AM = sum(amount_actual) / sum(predictions_glmnet)  # Calculate the Actual-to-Model ratio
    ) %>%
    ## Create a flextable from the summarized data
    flextable() %>%
    ## Format the Actual-to-Model column as percentages
    set_formatter(
      AM = function(x) {
        if (is.numeric(x))
          sprintf("%.1f%%", x * 100)
        else
          x
      }
    ) %>%
    ## Format the Outcome column as numbers
    colformat_num(j = "Outcome") %>%
    ## Set header labels for the table
    set_header_labels(
      Outcome = "Outcome",
      AM = "Actual-to-Model"
    ) %>%
    autofit() #%>%
    # Print the flextable
    #knitr::knit_print()
}) %>%
  ## Set names for each table based on the factor columns
  purrr::set_names(factor_cols) ->
  output_tables

if(output_format == "html") {
  output_tables %>%
    map(.f=knitr::knit_print) %>%
    ## Generate a tabset from the list of tables
    generate_tabset(
      tabtitle = "",
      tablevel = 4
    ) %>%
    ## Print the generated tabset
    cat()
} else {
  export_tables_to_excel(
    output_tables,
    paste0(
      exportsRoot,
      "_glmnet_int_univariate_goodness_of_fit_tables.xlsx"
    )
  )
  
  cat("See included Excel table for additional information.\n")
}
```

#### Bivariate Goodness-of-Fit {.tabset}

```{r glmnet-gof-bivariate, message=FALSE, warning=FALSE,results='asis'}
## Create a list of unique pairs of factor columns
pairlist <- data.table()
for (i in 1:(length(factor_cols) - 1)) {
  for (j in (i + 1):length(factor_cols)) {
    if (i == 1 & j == 2) {
      pairlist <- data.table(F1 = factor_cols[i], F2 = factor_cols[j])
    } else {
      pairlist <- rbind(pairlist, data.table(F1 = factor_cols[i], F2 = factor_cols[j]))
    }
  }
}

## Generate and format summary tables for each pair of factor columns
map2(.x = pairlist$F1, .y = pairlist$F2, .f = \(x, y) {
  xs <- sym(x)
  ys <- sym(y)
  
  ## Choose grouping order based on the number of levels in each factor
  if (length(train[, levels(get(x))]) >= length(train[, levels(get(y))])) {
    fttmp <- train %>%
      group_by(!!xs, !!ys) %>%
      summarize(
        Outcome = sum(amount_actual),
        Ratio = sprintf("%.1f%%", 100 * sum(amount_actual) / sum(predictions_glmnet))
      ) %>%
      pivot_wider(
        names_from = !!ys,
        values_from = c(Outcome, Ratio),
        names_glue = paste0(y, ": {", y, "}.{.value}"),
        names_vary = "slowest"
      )
  } else {
    fttmp <- train %>%
      group_by(!!ys, !!xs) %>%
      summarize(
        Outcome = sum(amount_actual),
        Ratio = sprintf("%.1f%%", 100 * sum(amount_actual) / sum(predictions_glmnet))
      ) %>%
      pivot_wider(
        names_from = !!xs,
        values_from = c(Outcome, Ratio),
        names_glue = paste0(x, ": {", x, "}.{.value}"),
        names_vary = "slowest"
      )
  }
  
  ## Adjust column keys for the flextable
  fttmp.colkeys <- names(fttmp)[1]
  for (i in 1:((length(names(fttmp)) - 1) / 2)) {
    fttmp.colkeys <- c(fttmp.colkeys, paste0("blank", i), names(fttmp)[(2 * i):(2 * i + 1)])
  }
  
  ## Create and print the flextable
  fttmp %>%
    flextable(col_keys = fttmp.colkeys) %>%
    ftExtra::span_header(sep = "\\.") %>%
    align(align = 'center', part = "all") %>%
    empty_blanks() %>%
    autofit() #%>%
    #knitr::knit_print()
}) %>%
  ## Set names for each element in the list based on the factor column pairs
  purrr::set_names(pairlist[, paste0(F1, " x ", F2)]) ->
  output_tables

if(output_format == "html") {
  output_tables %>%
    map(.f=knitr::knit_print) %>%
    ## Generate a tabset from the list of formatted tables
    generate_tabset(
      tabtitle = "",
      tablevel = 4
    ) %>%
    ## Print the generated tabset
    cat()
} else {
  export_tables_to_excel(
    output_tables,
    paste0(
      exportsRoot,
      "_glmnet_int_bivariate_goodness_of_fit_tables.xlsx"
    )
  )
  
  cat("See included Excel table for additional information.\n")
}

```
